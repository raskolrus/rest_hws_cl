{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ca41e2",
   "metadata": {},
   "source": [
    "# Домашнее задание № 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0588a6",
   "metadata": {},
   "source": [
    "### Задание 1 Реализовать алгоритм Леска и проверить его на реальном датасете (8 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a5283f",
   "metadata": {},
   "source": [
    "Ворднет можно использовать для дизамбигуации. Самый простой алгоритм дизамбигуации - алгоритм Леска. В нём нужное значение слова находится через пересечение слов контекста, в котором употреблено это слово, с определениями значений слова из ворднета. Значение с максимальным пересечением - нужное."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f082cd",
   "metadata": {},
   "source": [
    "Реализуйте его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c1c6fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33874a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \n",
    "    words = [token.text.strip(punct) for token in list(razdel_tokenize(text))]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a823f931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw-1.4.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff0052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bba5516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27a39f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0faf4ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'there', '!', 'msie', 'o', \"''\", 'braian']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize('hi there! msie o\"braian',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "397a41b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e497545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = punctuation+'«»—…“”*№–.!?-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "629f95bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    words = [token.strip(punct) for token in word_tokenize(text)]\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word and word not in stops]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b83499ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7cdae676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_context_wiki(sentence):\n",
    "    word_context = []\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i][0]:\n",
    "            word_context.append((sentence[i][1], sentence[i][0], ' '.join([word[1] for word in sentence if word[1]!=sentence[i][1] and word[1] not in punct])))\n",
    "    return word_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "668e6803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk( word, sentence ):\n",
    "    synsets = wn.synsets(word)\n",
    "\n",
    "    bestsense = 0\n",
    "    maxoverlap = 0\n",
    "    sentence = set(sentence.split())\n",
    "    \n",
    "    for i, syns in enumerate(synsets):\n",
    "        descr = set(normalize(syns.definition()))\n",
    "        if len(descr & sentence) > maxoverlap:\n",
    "            bestsense = i\n",
    "            maxoverlap = len(descr & sentence)\n",
    "\n",
    "        \n",
    "        \n",
    "    return bestsense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b352903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_context(word, sentence): #для удобства немного подредактировала функцию и принцип вывода\n",
    "    sentence = normalize(sentence)\n",
    "    sentence = [i for i in sentence if i!=word]\n",
    "    return word, ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1aa7ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_wsd = []\n",
    "corpus = open('corpus_wsd_50k.txt').read().split('\\n\\n')\n",
    "for sent in corpus:\n",
    "    corpus_wsd.append([s.split('\\t') for s in sent.split('\\n')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d5e58012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for numb, i in enumerate(corpus_wsd):\n",
    "        if numb % 10000==0:\n",
    "            print(numb)\n",
    "        ambiwords = get_words_in_context_wiki(i)\n",
    "        for q in ambiwords:\n",
    "            total+=1\n",
    "            word, gold, context = q\n",
    "            predicted = lesk(word, context)\n",
    "            correct += int(wn.lemma_from_key(gold).synset()==wn.synsets(word)[predicted])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856efa54",
   "metadata": {},
   "source": [
    "Вполне хороший результат для настолько наивного алгоритма! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "de4c29ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4306519446632738"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "24ab77f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48b45d4",
   "metadata": {},
   "source": [
    "### Задание 2* (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ef129",
   "metadata": {},
   "source": [
    "В семинаре для WSI на данных Диалога использовался только Fastext. Попробуйте заменить его на адаграм (обучите свою модель или используйте предобученную out.pkl или https://s3.amazonaws.com/kostia.lopuhin/all.a010.p10.d300.w5.m100.nonorm.slim.joblib), а также поэкспериментируйте с разными алгоритмами кластеризации и их параметрами (можно использовать только те алгоритмы, которые обучаются достаточно быстро)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89e5ea3",
   "metadata": {},
   "source": [
    "Для каждого эксперимента рассчитайте ARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1c17e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget 'https://s3.amazonaws.com/kostia.lopuhin/all.a010.p10.d300.w5.m100.nonorm.slim.joblib'\n",
    "vm = adagram.VectorModel.load('all.a010.p10.d300.w5.m100.nonorm.slim.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0006b5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vm.sense_vector('мир', vm.disambiguate(\"мир\", ['существование', \"космический\"]).argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "eaea66ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\OneDrive\\Рабочий стол\\учеба\\notebooks\\hse\\lib\\site-packages\\adagram\\model.py:124: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sim_matrix = np.dot(self.In, s_v) / self.InNorms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('заречье', 0, 0.6043546),\n",
       " ('сосновка', 0, 0.5947289),\n",
       " ('борок', 0, 0.5784992),\n",
       " ('волосово', 0, 0.572816),\n",
       " ('деревня', 5, 0.56453264),\n",
       " ('александровка', 0, 0.55563515),\n",
       " ('андреевка', 0, 0.5535462),\n",
       " ('поселок', 2, 0.5489396),\n",
       " ('роща', 0, 0.5486376),\n",
       " ('михайловка', 0, 0.5477818)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm.sense_neighbors('бор',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1642edf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "35755026",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "82fcf97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c3b3cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()\n",
    "\n",
    "rus_stops = set(stopwords.words('russian'))\n",
    "\n",
    "def normalize_rus(text):\n",
    "    \n",
    "    words = [token.strip(punct) for token in text.split()]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in rus_stops]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "da9bda4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['normalized_context']=df['context'].apply(normalize_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9ab9b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby('word')[['word', 'context', 'gold_sense_id', 'normalized_context']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0484b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "56096153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "14887859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9efabe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b4880362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6dd60e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_adagram(text, model, window, dim):\n",
    "    \n",
    "    \n",
    "    word2context = []\n",
    "    for i in range(len(text)-1):\n",
    "        left = max(0, i-window)\n",
    "        word = text[i]\n",
    "        left_context = text[left:i]\n",
    "        right_context = text[i+1:i+window]\n",
    "        context = left_context + right_context\n",
    "        word2context.append((word, context))\n",
    "    \n",
    "    \n",
    "    \n",
    "    vectors = np.zeros((len(word2context), dim))\n",
    "    \n",
    "    for i,word in enumerate(word2context):\n",
    "        word, context = word\n",
    "        try:\n",
    "            sense = model.disambiguate(word, context).argmax()\n",
    "            v = model.sense_vector(word, sense)\n",
    "            vectors[i] = v \n",
    "\n",
    "        except (KeyError):\n",
    "            continue\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dde476",
   "metadata": {},
   "source": [
    "Как я поняла, требуется векторизовать именно контекст слова,  а не напрямую дизамбигуировать его с помощью адаграмма (к слову, попытки сделать это принесли отрицательный ARI)\n",
    "\n",
    "Чтобы не извлекать вектора каждый раз, было решено провести все эксперименты в одном цикле."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "91f76fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARI = dict()\n",
    "\n",
    "for key, _ in grouped_df:\n",
    "\n",
    "    texts = grouped_df.get_group(key)['context']\n",
    "    X = np.zeros((len(texts), 300))\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        text = [word for word in text if word != key]\n",
    "        X[i] = get_embedding_adagram(text, vm, 10, 300)\n",
    "        \n",
    "    cluster_af_strict = AffinityPropagation(damping=0.9)\n",
    "    cluster_af_chill = AffinityPropagation(damping=0.6)\n",
    "    cluster_kmeans_2 = KMeans(2)\n",
    "    cluster_kmeans_3 = KMeans(3)\n",
    "    cluster_dbscan_more_clusters = DBSCAN(min_samples=3, eps=0.1)\n",
    "    cluster_dbscan_less_clusters = DBSCAN(min_samples=15, eps=0.3)\n",
    "    cluster_models = [cluster_af_strict, cluster_af_chill, cluster_kmeans_2, cluster_kmeans_3, cluster_dbscan_more_clusters, cluster_dbscan_less_clusters,]\n",
    "    cl_dict = dict().fromkeys([str(cl) for cl in cluster_models])\n",
    "    ARI[key]=cl_dict\n",
    "    for cl in cluster_models:\n",
    "        cl.fit(X)\n",
    "        labels = np.array(cl.labels_)+1\n",
    "        cl_dict[str(cl)]=adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels)\n",
    "    \n",
    "#print(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b6f90027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'бор': {'AffinityPropagation(damping=0.9)': 0.060499816543682985,\n",
       "  'AffinityPropagation(damping=0.6)': 0.060499816543682985,\n",
       "  'KMeans(n_clusters=2)': 0.021966527196652718,\n",
       "  'KMeans(n_clusters=3)': 0.05879342190177359,\n",
       "  'DBSCAN(eps=0.1, min_samples=3)': -0.013878836902379838,\n",
       "  'DBSCAN(eps=0.3, min_samples=15)': 0.0},\n",
       " 'замок': {'AffinityPropagation(damping=0.9)': 0.0,\n",
       "  'AffinityPropagation(damping=0.6)': 0.020073820116747968,\n",
       "  'KMeans(n_clusters=2)': 0.10852801674075456,\n",
       "  'KMeans(n_clusters=3)': 0.1279994083623275,\n",
       "  'DBSCAN(eps=0.1, min_samples=3)': 0.12965894306862152,\n",
       "  'DBSCAN(eps=0.3, min_samples=15)': 0.023480134669933688},\n",
       " 'лук': {'AffinityPropagation(damping=0.9)': 0.026059548909006133,\n",
       "  'AffinityPropagation(damping=0.6)': 0.017322267711951715,\n",
       "  'KMeans(n_clusters=2)': 0.05819292170507473,\n",
       "  'KMeans(n_clusters=3)': 0.06946293941409047,\n",
       "  'DBSCAN(eps=0.1, min_samples=3)': 0.09074873568109885,\n",
       "  'DBSCAN(eps=0.3, min_samples=15)': 0.05819292170507473},\n",
       " 'суда': {'AffinityPropagation(damping=0.9)': 0.015781583366240512,\n",
       "  'AffinityPropagation(damping=0.6)': 0.015781583366240512,\n",
       "  'KMeans(n_clusters=2)': 0.10134333893439433,\n",
       "  'KMeans(n_clusters=3)': 0.08439827507384248,\n",
       "  'DBSCAN(eps=0.1, min_samples=3)': 0.13024669361777738,\n",
       "  'DBSCAN(eps=0.3, min_samples=15)': -0.02687471586814064}}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc52a390",
   "metadata": {},
   "source": [
    "Способ складирования ARI в зависимости от алгоритма кластеризации мною был выбран не самый удачный, так что с помощью еще одного цикла настроим удобный принт. И наши финалисты:\n",
    "\n",
    "1) K-means(3)\n",
    "2) K-means(2)\n",
    "3) DBSCAN(eps=0.1, minsamples=3)\n",
    "\n",
    "В целом удалось улучшить результат аж в 8 раз, но показатель все еще мизерный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f368bac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AffinityPropagation(damping=0.9): 0.02558523720473241\n",
      "AffinityPropagation(damping=0.6): 0.028419371934655793\n",
      "KMeans(n_clusters=2): 0.07250770114421909\n",
      "KMeans(n_clusters=3): 0.08516351118800852\n",
      "DBSCAN(eps=0.1, min_samples=3): 0.08419388386627949\n",
      "DBSCAN(eps=0.3, min_samples=15): 0.013699585126716946\n"
     ]
    }
   ],
   "source": [
    "for i in cl_dict.keys():\n",
    "    aris = []\n",
    "    for key, value in ARI.items():\n",
    "        aris.append(value[i])\n",
    "    print(i+': '+str(np.mean(aris)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
